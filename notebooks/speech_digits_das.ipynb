{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RAdam' from 'optim' (../Processor\\optim.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-5fb45922fc21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0moptim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_processor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpeechDigitsDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBinningHistogram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RAdam' from 'optim' (../Processor\\optim.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../Processor\")\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from data_processor import SpeechDigitsDataset, BinningHistogram, Pad\n",
    "from models import SNN, SpikingConv2DLayer, ReadoutLayer, SurrogateHeaviside, SpikingDenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float16\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dct = {'o': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, 'z': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size =  1970\n",
      "Test dataset size =  497\n",
      "Maximum end time =  370.8927869796753\n"
     ]
    }
   ],
   "source": [
    "train_dataset_raw = SpeechDigitsDataset(data_root=\"\", mode=\"train\", train_proportion=0.8, label_dct=label_dct, transform = None, nb_digits=1)\n",
    "test_dataset_raw = SpeechDigitsDataset(data_root=\"\", mode=\"test\", train_proportion=0.8, label_dct=label_dct, transform = None, nb_digits=1)\n",
    "print(\"Train dataset size = \", len(train_dataset_raw))\n",
    "print(\"Test dataset size = \", len(test_dataset_raw))\n",
    "max_end_time = train_dataset_raw.get_max_end_time()\n",
    "print(\"Maximum end time = \", max_end_time / 0.005)\n",
    "size = 375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):    \n",
    "    X_batch = np.array([d[0] for d in data])\n",
    "    std = X_batch.std(axis=(0,2), keepdims=True)\n",
    "    std[std==0] = 1\n",
    "    X_batch = torch.tensor(X_batch/std)\n",
    "    y_batch = torch.tensor([d[1] for d in data])\n",
    "    return X_batch, y_batch    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"\"\n",
    "binning_method = \"time\"\n",
    "T_l = 0.005\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "binning = BinningHistogram(binning_method=binning_method, T_l=T_l)\n",
    "pad = Pad(size)\n",
    "transform = torchvision.transforms.Compose([binning,\n",
    "                                 pad])\n",
    "\n",
    "train_dataset = SpeechDigitsDataset(data_root, transform = transform, mode=\"train\", train_proportion=0.8, label_dct=label_dct, nb_digits=1)\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(train_dataset.weights,len(train_dataset.weights))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = SpeechDigitsDataset(data_root, transform = transform, mode=\"test\", train_proportion=0.8, label_dct=label_dct, nb_digits=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data:  1970\n",
      "Size of test data:  497\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training data: \", len(train_dataset))\n",
    "print(\"Size of test data: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: average number of spikes=0.0327\n",
      "Layer 1: average number of spikes=0.0344\n",
      "Layer 2: average number of spikes=0.0440\n"
     ]
    }
   ],
   "source": [
    "spike_fn = SurrogateHeaviside.apply\n",
    "\n",
    "w_init_std = 0.15\n",
    "w_init_mean = 0.\n",
    "\n",
    "layers = []\n",
    "in_channels = 1\n",
    "out_channels = 64\n",
    "kernel_size = (4,3)\n",
    "dilation = (1,1)\n",
    "input_shape = 64\n",
    "output_shape = input_shape # padding mode is \"same\"\n",
    "layers.append(SpikingConv2DLayer(input_shape, output_shape,\n",
    "                 in_channels, out_channels, kernel_size, dilation,\n",
    "                 spike_fn, w_init_mean=w_init_mean, w_init_std=w_init_std, recurrent=False,\n",
    "                               lateral_connections=True))\n",
    "\n",
    "\n",
    "in_channels = out_channels\n",
    "out_channels = 64\n",
    "kernel_size = (4,3)\n",
    "dilation = (4,3)\n",
    "input_shape = output_shape\n",
    "output_shape = input_shape # padding mode is \"same\"\n",
    "layers.append(SpikingConv2DLayer(input_shape, output_shape,\n",
    "                 in_channels, out_channels, kernel_size, dilation,\n",
    "                 spike_fn, w_init_mean=w_init_mean, w_init_std=w_init_std, recurrent=False,\n",
    "                              lateral_connections=True))\n",
    "\n",
    "in_channels = out_channels\n",
    "out_channels = 64\n",
    "kernel_size = (4,3)\n",
    "dilation = (16,9)\n",
    "input_shape = output_shape\n",
    "output_shape = input_shape # padding mode is \"same\"\n",
    "layers.append(SpikingConv2DLayer(input_shape, output_shape,\n",
    "                 in_channels, out_channels, kernel_size, dilation,\n",
    "                 spike_fn, w_init_mean=w_init_mean, w_init_std=w_init_std, recurrent=False,\n",
    "                               lateral_connections=True, flatten_output=True))\n",
    "\n",
    "# previous layer output has been flattened\n",
    "input_shape = output_shape*out_channels\n",
    "output_shape = 10\n",
    "time_reduction=\"mean\" #mean or max\n",
    "layers.append(ReadoutLayer(input_shape, output_shape,\n",
    "                 w_init_mean=w_init_mean, w_init_std=w_init_std, time_reduction=time_reduction))\n",
    "\n",
    "snn = SNN(layers).to(device, dtype)\n",
    "\n",
    "X_batch, _ = next(iter(train_dataloader))\n",
    "X_batch = X_batch.to(device, dtype)\n",
    "#you need to add a channel dimension\n",
    "X_batch = X_batch.unsqueeze(1)\n",
    "snn(X_batch)\n",
    "\n",
    "for i,l in enumerate(snn.layers):\n",
    "    if isinstance(l, SpikingDenseLayer) or isinstance(l, SpikingConv2DLayer):\n",
    "        print(\"Layer {}: average number of spikes={:.4f}\".format(i,l.spk_rec_hist.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, params, optimizer, train_dataloader, valid_dataloader, reg_loss_coef, nb_epochs, scheduler=None, warmup_epochs=0):\n",
    "    \n",
    "    log_softmax_fn = torch.nn.LogSoftmax(dim=1)\n",
    "    loss_fn = torch.nn.NLLLoss()\n",
    "    \n",
    "    if warmup_epochs > 0:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] /= len(train_dataloader)*warmup_epochs\n",
    "        warmup_itr = 1\n",
    "    \n",
    "    hist = {'loss':[], 'valid_accuracy':[]}\n",
    "    for e in tqdm(range(nb_epochs)):\n",
    "        local_loss = []\n",
    "        reg_loss = [[] for _ in range(len(model.layers)-1)]\n",
    "        \n",
    "        for x_batch, y_batch in tqdm(train_dataloader):\n",
    "            x_batch = x_batch.to(device, dtype)\n",
    "            x_batch = x_batch.unsqueeze(1)\n",
    "            y_batch = y_batch.float().to(device)\n",
    "\n",
    "            output, loss_seq = model(x_batch)\n",
    "            log_p_y = log_softmax_fn(output)\n",
    "            loss_val = loss_fn(log_p_y, y_batch.long())\n",
    "            local_loss.append(loss_val.item())\n",
    "\n",
    "            for i,loss in enumerate(loss_seq[:-1]):\n",
    "                reg_loss_val = reg_loss_coef*loss*(i+1)/len(loss_seq[:-1])\n",
    "                loss_val += reg_loss_val\n",
    "                reg_loss[i].append(reg_loss_val.item())\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            torch.nn.utils.clip_grad_value_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            model.clamp()\n",
    "\n",
    "            if e < warmup_epochs:\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] *= (warmup_itr+1)/(warmup_itr)\n",
    "                warmup_itr += 1\n",
    "                \n",
    "                #pb.update(1)\n",
    "                \n",
    "        if scheduler is not None and e >= warmup_epochs:\n",
    "            scheduler.step()\n",
    "        \n",
    "        mean_loss = np.mean(local_loss)\n",
    "        hist['loss'].append(mean_loss)\n",
    "        print(\"Epoch %i: loss=%.5f\"%(e+1,mean_loss))\n",
    "        \n",
    "        for i,loss in enumerate(reg_loss):\n",
    "            mean_reg_loss = np.mean(loss)\n",
    "            print(\"Layer %i: reg loss=%.5f\"%(i,mean_reg_loss))\n",
    "            \n",
    "        for i,l in enumerate(snn.layers[:-1]):\n",
    "            print(\"Layer {}: average number of spikes={:.4f}\".format(i,l.spk_rec_hist.mean()))\n",
    "        \n",
    "        valid_accuracy = compute_classification_accuracy(model, valid_dataloader)\n",
    "        hist['valid_accuracy'].append(valid_accuracy)\n",
    "        print(\"Validation accuracy=%.3f\"%(valid_accuracy))\n",
    "        \n",
    "    return hist\n",
    "        \n",
    "def compute_classification_accuracy(model, dataloader):\n",
    "    accs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #with tqdm_notebook(total=len(dataloader)) as pb:\n",
    "        for x_batch, y_batch in dataloader:\n",
    "\n",
    "            x_batch = x_batch.to(device, dtype)\n",
    "            x_batch = x_batch.unsqueeze(1)\n",
    "            y_batch = y_batch.float().to(device)\n",
    "            output, _ = model(x_batch)\n",
    "            _,am=torch.max(output,1) # argmax over output units\n",
    "            tmp = np.mean((y_batch==am).detach().cpu().numpy()) # compare to labels\n",
    "            accs.append(tmp)\n",
    "                #pb.update(1)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jaggbow\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c198fe78cd2a43c89c101589bd9967fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jaggbow\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6edf7dc4a34c1f8f46ff89c0f6448f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: loss=1.74690\n",
      "Layer 0: reg loss=0.00054\n",
      "Layer 1: reg loss=0.00111\n",
      "Layer 2: reg loss=0.00214\n",
      "Layer 0: average number of spikes=0.0370\n",
      "Layer 1: average number of spikes=0.0339\n",
      "Layer 2: average number of spikes=0.0514\n",
      "Validation accuracy=0.574\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e54ee6dff904df89fd84bcb214b7c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: loss=0.81775\n",
      "Layer 0: reg loss=0.00054\n",
      "Layer 1: reg loss=0.00092\n",
      "Layer 2: reg loss=0.00190\n",
      "Layer 0: average number of spikes=0.0363\n",
      "Layer 1: average number of spikes=0.0278\n",
      "Layer 2: average number of spikes=0.0414\n",
      "Validation accuracy=0.732\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3892528f37f94c069fb552583d8583dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: loss=0.55747\n",
      "Layer 0: reg loss=0.00053\n",
      "Layer 1: reg loss=0.00087\n",
      "Layer 2: reg loss=0.00185\n",
      "Layer 0: average number of spikes=0.0356\n",
      "Layer 1: average number of spikes=0.0255\n",
      "Layer 2: average number of spikes=0.0326\n",
      "Validation accuracy=0.756\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89330b996f8465dbfa078240512e9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: loss=0.43407\n",
      "Layer 0: reg loss=0.00052\n",
      "Layer 1: reg loss=0.00089\n",
      "Layer 2: reg loss=0.00183\n",
      "Layer 0: average number of spikes=0.0355\n",
      "Layer 1: average number of spikes=0.0258\n",
      "Layer 2: average number of spikes=0.0330\n",
      "Validation accuracy=0.781\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1a58c32dea4732bb2a4dbb34ba1529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: loss=0.37642\n",
      "Layer 0: reg loss=0.00051\n",
      "Layer 1: reg loss=0.00090\n",
      "Layer 2: reg loss=0.00175\n",
      "Layer 0: average number of spikes=0.0351\n",
      "Layer 1: average number of spikes=0.0268\n",
      "Layer 2: average number of spikes=0.0324\n",
      "Validation accuracy=0.807\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f037a7292bd841c19facbb5b2f159631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: loss=0.35278\n",
      "Layer 0: reg loss=0.00051\n",
      "Layer 1: reg loss=0.00089\n",
      "Layer 2: reg loss=0.00166\n",
      "Layer 0: average number of spikes=0.0351\n",
      "Layer 1: average number of spikes=0.0281\n",
      "Layer 2: average number of spikes=0.0328\n",
      "Validation accuracy=0.801\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a98d5fcefd461f8c45fdd8d28e3102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: loss=0.30967\n",
      "Layer 0: reg loss=0.00051\n",
      "Layer 1: reg loss=0.00090\n",
      "Layer 2: reg loss=0.00164\n",
      "Layer 0: average number of spikes=0.0349\n",
      "Layer 1: average number of spikes=0.0269\n",
      "Layer 2: average number of spikes=0.0297\n",
      "Validation accuracy=0.844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45954c5f04f4ae5ab78405e0b6e30f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: loss=0.26839\n",
      "Layer 0: reg loss=0.00051\n",
      "Layer 1: reg loss=0.00091\n",
      "Layer 2: reg loss=0.00163\n",
      "Layer 0: average number of spikes=0.0349\n",
      "Layer 1: average number of spikes=0.0287\n",
      "Layer 2: average number of spikes=0.0300\n",
      "Validation accuracy=0.852\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bfcb8b71904011aa12aefc43deb01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: loss=0.23337\n",
      "Layer 0: reg loss=0.00051\n",
      "Layer 1: reg loss=0.00091\n",
      "Layer 2: reg loss=0.00162\n",
      "Layer 0: average number of spikes=0.0349\n",
      "Layer 1: average number of spikes=0.0293\n",
      "Layer 2: average number of spikes=0.0297\n",
      "Validation accuracy=0.840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd22f8476ff4642806531ca0fd6a519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: loss=0.20823\n",
      "Layer 0: reg loss=0.00051\n",
      "Layer 1: reg loss=0.00092\n",
      "Layer 2: reg loss=0.00162\n",
      "Layer 0: average number of spikes=0.0349\n",
      "Layer 1: average number of spikes=0.0290\n",
      "Layer 2: average number of spikes=0.0309\n",
      "Validation accuracy=0.850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "weight_decay = 1e-5\n",
    "reg_loss_coef = 0.1\n",
    "nb_epochs = 10\n",
    "\n",
    "params = [{'params':l.w, 'lr':lr, \"weight_decay\":weight_decay } for i,l in enumerate(snn.layers)]\n",
    "params += [{'params':l.v, 'lr':lr, \"weight_decay\":weight_decay} for i,l in enumerate(snn.layers[:-1]) if l.recurrent]\n",
    "params += [{'params':l.b, 'lr':lr} for i,l in enumerate(snn.layers)]\n",
    "if snn.layers[-1].time_reduction == \"mean\":\n",
    "    params += [{'params':l.beta, 'lr':lr} for i,l in enumerate(snn.layers[:-1])]\n",
    "elif snn.layers[-1].time_reduction == \"max\":\n",
    "    params += [{'params':l.beta, 'lr':lr} for i,l in enumerate(snn.layers)]\n",
    "else:\n",
    "    raise ValueError(\"Readout time recution should be 'max' or 'mean'\")\n",
    "    \n",
    "optimizer = RAdam(params)\n",
    " \n",
    "gamma = 0.85\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)\n",
    "\n",
    "hist = train(snn, params, optimizer, train_dataloader, test_dataloader, reg_loss_coef, nb_epochs=nb_epochs,\n",
    "                  scheduler=scheduler, warmup_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
